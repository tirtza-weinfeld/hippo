# Training Scripts

Local training workflow for neural networks. Train models with full control, then upload to Hugging Face Hub for API use.

## Quick Start

```bash
# 1. Train a model
python training/cli_train.py \
    --sizes 784 100 10 \
    --activation relu \
    --epochs 30 \
    --learning-rate 3.0

# 2. Upload to HF Hub
python training/cli_upload.py \
    --model-path models/mnist-relu-100-20251114_103000.npz \
    --accuracy 95.4

# 3. Update API config
echo "DEFAULT_MODEL=mnist-relu-100-20251114_103000" >> ../.env

# 4. Restart API
cd .. && uvicorn api.main:app --reload
```

## Scripts

### Core Logic (Importable)

- **`train.py`** - Pure training functions, no CLI dependencies
- **`upload.py`** - Pure upload functions, no CLI dependencies

### CLI Wrappers

- **`cli_train.py`** - Command-line interface for training
- **`cli_upload.py`** - Command-line interface for uploading

### `cli_train.py`

Train feedforward neural networks on MNIST dataset.

**Options:**
- `--sizes`: Layer architecture (e.g., `784 100 10`)
- `--activation`: `sigmoid` or `relu`
- `--epochs`: Number of training epochs
- `--learning-rate`: Learning rate (typically 0.1-10.0)
- `--mini-batch-size`: Mini-batch size (default: 10)
- `--output`: Output file path (auto-generated if not provided)
- `--seed`: Random seed for reproducibility

**Examples:**

```bash
# Small network, quick training
python training/cli_train.py --sizes 784 30 10 --epochs 10

# Large network, high accuracy
python training/cli_train.py --sizes 784 128 64 10 --activation relu --epochs 50

# Reproducible training
python training/cli_train.py --sizes 784 100 10 --seed 42
```

### `cli_upload.py`

Upload trained models to Hugging Face Hub with metadata.

**Options:**
- `--model-path`: Path to `.npz` model file (required)
- `--accuracy`: Test accuracy percentage (required)
- `--name`: Model name (default: filename)
- `--description`: Model description
- `--epochs`: Training epochs (auto-extracted if available)
- `--repo-id`: HF Hub repo (default: from `HF_MODEL_REPO` env var)

**Examples:**

```bash
# Basic upload
python training/cli_upload.py \
    --model-path models/my-model.npz \
    --accuracy 95.4

# With description
python training/cli_upload.py \
    --model-path models/my-model.npz \
    --name "mnist-relu-best" \
    --description "Best performing ReLU model" \
    --accuracy 96.2
```

## Directory Structure

```
training/
├── train.py                # Pure training logic (importable)
├── upload.py               # Pure upload logic (importable)
├── cli_train.py            # CLI wrapper for training
├── cli_upload.py           # CLI wrapper for uploading
├── experiments/            # Jupyter notebooks for exploration
│   └── train_interactive.ipynb
└── README.md               # This file

models/                     # Generated by training scripts
├── mnist-relu-100-*.npz    # Trained model files
└── *.json                  # Model metadata
```

## Environment Setup

```bash
# Required for uploads
export HUGGINGFACE_TOKEN=hf_...
export HF_MODEL_REPO=your-username/hippo-models

# Or add to .env
echo "HUGGINGFACE_TOKEN=hf_..." >> ../.env
echo "HF_MODEL_REPO=your-username/hippo-models" >> ../.env
```

## Workflow

1. **Train**: Run `train_mnist.py` with desired hyperparameters
2. **Evaluate**: Script shows final accuracy on test set
3. **Save**: Model saved as `.npz` in `models/` directory
4. **Upload**: Use `upload_to_hf.py` to push to HF Hub
5. **Deploy**: Update `.env` and restart API

## Model File Format

Trained models are saved as NumPy compressed archives (`.npz`):

```python
{
    'weights': list[list[list[float]]],  # Layer weights
    'biases': list[list[list[float]]],   # Layer biases
    'sizes': list[int],                   # Layer sizes
    'activation': str,                    # Activation function
    'training_config': dict,              # Training hyperparameters
    'final_accuracy': float               # Test accuracy %
}
```

## Tips

### Finding Good Hyperparameters

```bash
# Start small to iterate quickly
python training/cli_train.py --sizes 784 30 10 --epochs 10

# Once you find good settings, train longer
python training/cli_train.py --sizes 784 30 10 --epochs 50

# Try different activations
python training/cli_train.py --sizes 784 100 10 --activation sigmoid
python training/cli_train.py --sizes 784 100 10 --activation relu
```

### Experiment Tracking

Use Jupyter notebooks in `experiments/` for:
- Comparing multiple architectures
- Plotting training curves
- Analyzing learned weights
- Testing different learning rates

### Model Naming

Use descriptive names that include key info:

```
mnist-{activation}-{hidden_sizes}-{accuracy}acc.npz

Examples:
mnist-relu-100-95acc.npz
mnist-sigmoid-30-92acc.npz
mnist-relu-128x64-96acc.npz
```

## Troubleshooting

**Import errors:**
```bash
# Run from project root
cd /path/to/hippo
python training/cli_train.py ...
```

**HF Hub upload fails:**
```bash
# Login to HF Hub
huggingface-cli login

# Check token
echo $HUGGINGFACE_TOKEN
```

**Low accuracy:**
- Try ReLU activation
- Increase hidden layer size
- Train for more epochs
- Adjust learning rate

## Next Steps

See main docs for more details:
- [`docs/TRAINING.md`](../docs/TRAINING.md) - Detailed training guide
- [`docs/ARCHITECTURE.md`](../docs/ARCHITECTURE.md) - System architecture
- [`docs/TRAINING_CONCEPTS.md`](../docs/TRAINING_CONCEPTS.md) - How training works
